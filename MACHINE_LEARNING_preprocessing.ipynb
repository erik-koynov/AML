{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data / train-test split into separate files / store the embeddings into npy array for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import keras\n",
    "import pickle\n",
    "import spacy\n",
    "import threading\n",
    "from time import time\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "import gensim\n",
    "import pickle\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('disk/ERIK_ML/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "titles = []\n",
    "y = []\n",
    "for file_name in os.listdir('DataSet/'):\n",
    "    if not file_name.endswith('.json'):\n",
    "        continue\n",
    "    else:\n",
    "        file = open(os.path.join('DataSet/',file_name),'r')\n",
    "        data = json.load(file)\n",
    "        # extract questions\n",
    "        for line in data:\n",
    "            X.append(line['question'])\n",
    "            y.append(file_name[:-5])\n",
    "            titles.append(line['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hallo,\\n1) es existiert ein neu gegründetes Gewerbe (Einzelunternehmer). Es soll nun ein Fahrzeug geleast werden und komplett abgesetzt werden (sämtliche Kosten, Leasingrate, Leasingsonderzahlung, Benzin, Vorsteuer, usw.) Die Bank kann das Leasingangebot nur privat ausstellen, gewerblich geht dies nicht, da für das neu gegründete Gewerbe keine Zahlen vorliegen. Kann ein auf die Privatperson abgeschlossener Leasingvertrag trotzdem vollständig ins Gewerbe eingebracht werden, um alles abzusetzen? Im Leasingvertrag steht ausdrücklich „Privatleasing\". Oder geht dies nur mit einem gewerblichen Leasingvertrag?\\n2) Falls möglich - gäbe es hier ein Risiko im Falle einer Betriebsprüfung, da der Leasingvertrag als „Privatleasing\" deklariert ist, sodass die Betriebsausgaben nicht anerkannt werden o. Ä.?\\nVielen Dank.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Privates Leasing im Gewerbe?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_output = deepcopy(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;.\\\\n]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^A-Za-züäößÖÄÜ ]')\n",
    "\n",
    "PHRASES=re.compile(r\"\\bgute?n? tag\\b|\\bsehr geehrte damen und herren\\b|\\bha?e?llo\\b|\\bviele? grü?u?s?s?ß?e?\\b|\\bgute?n? morgen\\b|\\bdanke\\b|\\bbitten?\\b|\\bgar\\b\",\n",
    "                   re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('stopwords.pkl','rb')\n",
    "STOPWORDS = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im',\n",
       " 'und',\n",
       " 'diese',\n",
       " 'sind',\n",
       " 'ich',\n",
       " 'kommen',\n",
       " 'habe',\n",
       " 'damit',\n",
       " 'leben',\n",
       " 'man',\n",
       " 'möglich',\n",
       " 'euro',\n",
       " 'das',\n",
       " 'unter',\n",
       " 'mein',\n",
       " 'sein',\n",
       " 'mutter',\n",
       " 'wenn',\n",
       " 'jahr',\n",
       " 'jahren',\n",
       " 'dies',\n",
       " 'mal',\n",
       " 'zu',\n",
       " 'bekommen',\n",
       " 'oder',\n",
       " 'auf',\n",
       " 'für',\n",
       " 'hier',\n",
       " 'ohne',\n",
       " 'neu',\n",
       " 'so',\n",
       " 'noch',\n",
       " 'sagen',\n",
       " 'vater',\n",
       " 'war',\n",
       " 'kein',\n",
       " 'welch',\n",
       " 'monat',\n",
       " 'mich',\n",
       " 'mehr',\n",
       " 'hatte',\n",
       " 'seit',\n",
       " 'dass',\n",
       " 'meine',\n",
       " 'soll',\n",
       " 'die',\n",
       " 'ist',\n",
       " 'ein',\n",
       " 'lassen',\n",
       " 'er',\n",
       " 'geben',\n",
       " 'vom',\n",
       " 'den',\n",
       " 'haben',\n",
       " 'wie',\n",
       " 'bereits',\n",
       " 'zahlen',\n",
       " 'um',\n",
       " 'viel',\n",
       " 'verkaufen',\n",
       " 'es',\n",
       " 'einer',\n",
       " 'auch',\n",
       " 'nur',\n",
       " 'in',\n",
       " 'hausen',\n",
       " 'machen',\n",
       " 'folgend',\n",
       " 'da',\n",
       " 'eine',\n",
       " 'deutschland',\n",
       " 'wegen',\n",
       " '1',\n",
       " 'dieser',\n",
       " 'wohnung',\n",
       " 'nicht',\n",
       " 'meiner',\n",
       " 'sich',\n",
       " 'aus',\n",
       " 'einen',\n",
       " 'wird',\n",
       " 'nichts',\n",
       " 'sehen',\n",
       " 'vor',\n",
       " 'muss',\n",
       " 'von',\n",
       " 'zeit',\n",
       " 'ja',\n",
       " 'bei',\n",
       " 'ca',\n",
       " 'bin',\n",
       " 'der',\n",
       " 'also',\n",
       " 'werden',\n",
       " 'hat',\n",
       " 'jahre',\n",
       " 'recht',\n",
       " 'durch',\n",
       " 'über',\n",
       " 'bis',\n",
       " 'als',\n",
       " 'uns',\n",
       " 'dann',\n",
       " 'mit',\n",
       " 'immer',\n",
       " 'vertragen',\n",
       " 'schreiben',\n",
       " 'dürfen',\n",
       " 'erst',\n",
       " 'tochter',\n",
       " 'alle',\n",
       " 'tagen',\n",
       " 'gut',\n",
       " 'anwalt',\n",
       " 'fall',\n",
       " 'dank',\n",
       " 'dem',\n",
       " 'müssen',\n",
       " 'ab',\n",
       " 'wieder',\n",
       " 'gegen',\n",
       " 'gehen',\n",
       " 'sollen',\n",
       " 'schon',\n",
       " 'mir',\n",
       " 'weil',\n",
       " 'ob',\n",
       " '2',\n",
       " 'zum',\n",
       " 'wurde',\n",
       " 'bezahlen',\n",
       " 'nach',\n",
       " 'jedoch',\n",
       " 'möchte',\n",
       " 'keine',\n",
       " 'würde',\n",
       " 'stehen',\n",
       " 'wollen',\n",
       " 'des',\n",
       " 'wir',\n",
       " 'können',\n",
       " 'alt',\n",
       " 'aber',\n",
       " 'nun',\n",
       " 'kosten',\n",
       " 'kaufen',\n",
       " 'all',\n",
       " 'zur',\n",
       " 'frage',\n",
       " 'geld',\n",
       " 'woche',\n",
       " 'kann',\n",
       " 'stellen',\n",
       " 'arbeiten',\n",
       " 'frau',\n",
       " 'an',\n",
       " 'am',\n",
       " 'einem',\n",
       " 'sie',\n",
       " 'sehr',\n",
       " 'kind',\n",
       " 'erhalten',\n",
       " 'jetzt',\n",
       " 'weit',\n",
       " 'dort',\n",
       " '3',\n",
       " 'meinem',\n",
       " 'was',\n",
       " 'wo',\n",
       " 'hin',\n",
       " 'a',\n",
       " 'ab',\n",
       " 'aber',\n",
       " 'ach',\n",
       " 'acht',\n",
       " 'achte',\n",
       " 'achten',\n",
       " 'achter',\n",
       " 'achtes',\n",
       " 'ag',\n",
       " 'alle',\n",
       " 'allein',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'allerdings',\n",
       " 'alles',\n",
       " 'allgemeinen',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'au',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'ausser',\n",
       " 'ausserdem',\n",
       " 'außer',\n",
       " 'außerdem',\n",
       " 'b',\n",
       " 'bald',\n",
       " 'bei',\n",
       " 'beide',\n",
       " 'beiden',\n",
       " 'beim',\n",
       " 'beispiel',\n",
       " 'bekannt',\n",
       " 'bereits',\n",
       " 'besonders',\n",
       " 'besser',\n",
       " 'besten',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bisher',\n",
       " 'bist',\n",
       " 'c',\n",
       " 'd',\n",
       " 'd.h',\n",
       " 'da',\n",
       " 'dabei',\n",
       " 'dadurch',\n",
       " 'dafür',\n",
       " 'dagegen',\n",
       " 'daher',\n",
       " 'dahin',\n",
       " 'dahinter',\n",
       " 'damals',\n",
       " 'damit',\n",
       " 'danach',\n",
       " 'daneben',\n",
       " 'dank',\n",
       " 'dann',\n",
       " 'daran',\n",
       " 'darauf',\n",
       " 'daraus',\n",
       " 'darf',\n",
       " 'darfst',\n",
       " 'darin',\n",
       " 'darum',\n",
       " 'darunter',\n",
       " 'darüber',\n",
       " 'das',\n",
       " 'dasein',\n",
       " 'daselbst',\n",
       " 'dass',\n",
       " 'dasselbe',\n",
       " 'davon',\n",
       " 'davor',\n",
       " 'dazu',\n",
       " 'dazwischen',\n",
       " 'daß',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'dem',\n",
       " 'dementsprechend',\n",
       " 'demgegenüber',\n",
       " 'demgemäss',\n",
       " 'demgemäß',\n",
       " 'demselben',\n",
       " 'demzufolge',\n",
       " 'den',\n",
       " 'denen',\n",
       " 'denn',\n",
       " 'denselben',\n",
       " 'der',\n",
       " 'deren',\n",
       " 'derer',\n",
       " 'derjenige',\n",
       " 'derjenigen',\n",
       " 'dermassen',\n",
       " 'dermaßen',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'des',\n",
       " 'deshalb',\n",
       " 'desselben',\n",
       " 'dessen',\n",
       " 'deswegen',\n",
       " 'dich',\n",
       " 'die',\n",
       " 'diejenige',\n",
       " 'diejenigen',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'dir',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'drei',\n",
       " 'drin',\n",
       " 'dritte',\n",
       " 'dritten',\n",
       " 'dritter',\n",
       " 'drittes',\n",
       " 'du',\n",
       " 'durch',\n",
       " 'durchaus',\n",
       " 'durfte',\n",
       " 'durften',\n",
       " 'dürfen',\n",
       " 'dürft',\n",
       " 'e',\n",
       " 'eben',\n",
       " 'ebenso',\n",
       " 'ehrlich',\n",
       " 'ei',\n",
       " 'ei,',\n",
       " 'eigen',\n",
       " 'eigene',\n",
       " 'eigenen',\n",
       " 'eigener',\n",
       " 'eigenes',\n",
       " 'ein',\n",
       " 'einander',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'eins',\n",
       " 'elf',\n",
       " 'en',\n",
       " 'ende',\n",
       " 'endlich',\n",
       " 'entweder',\n",
       " 'er',\n",
       " 'ernst',\n",
       " 'erst',\n",
       " 'erste',\n",
       " 'ersten',\n",
       " 'erster',\n",
       " 'erstes',\n",
       " 'es',\n",
       " 'etwa',\n",
       " 'etwas',\n",
       " 'euch',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'f',\n",
       " 'folgende',\n",
       " 'früher',\n",
       " 'fünf',\n",
       " 'fünfte',\n",
       " 'fünften',\n",
       " 'fünfter',\n",
       " 'fünftes',\n",
       " 'für',\n",
       " 'g',\n",
       " 'gab',\n",
       " 'ganz',\n",
       " 'ganze',\n",
       " 'ganzen',\n",
       " 'ganzer',\n",
       " 'ganzes',\n",
       " 'gar',\n",
       " 'gedurft',\n",
       " 'gegen',\n",
       " 'gegenüber',\n",
       " 'gehabt',\n",
       " 'gehen',\n",
       " 'geht',\n",
       " 'gekannt',\n",
       " 'gekonnt',\n",
       " 'gemacht',\n",
       " 'gemocht',\n",
       " 'gemusst',\n",
       " 'genug',\n",
       " 'gerade',\n",
       " 'gern',\n",
       " 'gesagt',\n",
       " 'geschweige',\n",
       " 'gewesen',\n",
       " 'gewollt',\n",
       " 'geworden',\n",
       " 'gibt',\n",
       " 'ging',\n",
       " 'gleich',\n",
       " 'gott',\n",
       " 'gross',\n",
       " 'grosse',\n",
       " 'grossen',\n",
       " 'grosser',\n",
       " 'grosses',\n",
       " 'groß',\n",
       " 'große',\n",
       " 'großen',\n",
       " 'großer',\n",
       " 'großes',\n",
       " 'gut',\n",
       " 'gute',\n",
       " 'guter',\n",
       " 'gutes',\n",
       " 'h',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'habt',\n",
       " 'hast',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hattest',\n",
       " 'hattet',\n",
       " 'heisst',\n",
       " 'her',\n",
       " 'heute',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'hoch',\n",
       " 'hätte',\n",
       " 'hätten',\n",
       " 'i',\n",
       " 'ich',\n",
       " 'ihm',\n",
       " 'ihn',\n",
       " 'ihnen',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'im',\n",
       " 'immer',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'infolgedessen',\n",
       " 'ins',\n",
       " 'irgend',\n",
       " 'ist',\n",
       " 'j',\n",
       " 'ja',\n",
       " 'jahr',\n",
       " 'jahre',\n",
       " 'jahren',\n",
       " 'je',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedermann',\n",
       " 'jedermanns',\n",
       " 'jedes',\n",
       " 'jedoch',\n",
       " 'jemand',\n",
       " 'jemandem',\n",
       " 'jemanden',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'k',\n",
       " 'kam',\n",
       " 'kann',\n",
       " 'kannst',\n",
       " 'kaum',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'kleine',\n",
       " 'kleinen',\n",
       " 'kleiner',\n",
       " 'kleines',\n",
       " 'kommen',\n",
       " 'kommt',\n",
       " 'konnte',\n",
       " 'konnten',\n",
       " 'kurz',\n",
       " 'können',\n",
       " 'könnt',\n",
       " 'könnte',\n",
       " 'l',\n",
       " 'lang',\n",
       " 'lange',\n",
       " 'leicht',\n",
       " 'leide',\n",
       " 'lieber',\n",
       " 'los',\n",
       " 'm',\n",
       " 'machen',\n",
       " 'macht',\n",
       " 'machte',\n",
       " 'mag',\n",
       " 'magst',\n",
       " 'mahn',\n",
       " 'mal',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mann',\n",
       " 'mehr',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mensch',\n",
       " 'menschen',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'mit',\n",
       " 'mittel',\n",
       " 'mochte',\n",
       " 'mochten',\n",
       " 'morgen',\n",
       " 'muss',\n",
       " 'musst',\n",
       " 'musste',\n",
       " 'mussten',\n",
       " 'muß',\n",
       " 'mußt',\n",
       " 'möchte',\n",
       " 'mögen',\n",
       " 'möglich',\n",
       " 'mögt',\n",
       " 'müssen',\n",
       " 'müsst',\n",
       " 'müßt',\n",
       " 'n',\n",
       " 'na',\n",
       " 'nach',\n",
       " 'nachdem',\n",
       " 'nahm',\n",
       " 'natürlich',\n",
       " 'neben',\n",
       " 'nein',\n",
       " 'neue',\n",
       " 'neuen',\n",
       " 'neun',\n",
       " 'neunte',\n",
       " 'neunten',\n",
       " 'neunter',\n",
       " 'neuntes',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'nie',\n",
       " 'niemand',\n",
       " 'niemandem',\n",
       " 'niemanden',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'o',\n",
       " 'ob',\n",
       " 'oben',\n",
       " 'oder',\n",
       " 'offen',\n",
       " 'oft',\n",
       " 'ohne',\n",
       " 'ordnung',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 'recht',\n",
       " 'rechte',\n",
       " 'rechten',\n",
       " 'rechter',\n",
       " 'rechtes',\n",
       " 'richtig',\n",
       " 'rund',\n",
       " 's',\n",
       " 'sa',\n",
       " 'sache',\n",
       " 'sagt',\n",
       " 'sagte',\n",
       " 'sah',\n",
       " 'satt',\n",
       " 'schlecht',\n",
       " 'schluss',\n",
       " 'schon',\n",
       " 'sechs',\n",
       " 'sechste',\n",
       " 'sechsten',\n",
       " 'sechster',\n",
       " 'sechstes',\n",
       " 'sehr',\n",
       " 'sei',\n",
       " 'seid',\n",
       " 'seien',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'seit',\n",
       " 'seitdem',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'sieben',\n",
       " 'siebente',\n",
       " 'siebenten',\n",
       " 'siebenter',\n",
       " 'siebentes',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solang',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollen',\n",
       " 'sollst',\n",
       " 'sollt',\n",
       " 'sollte',\n",
       " 'sollten',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'soweit',\n",
       " 'sowie',\n",
       " 'später',\n",
       " 'startseite',\n",
       " 'statt',\n",
       " 'steht',\n",
       " 'suche',\n",
       " 't',\n",
       " 'tag',\n",
       " 'tage',\n",
       " 'tagen',\n",
       " 'tat',\n",
       " 'teil',\n",
       " 'tel',\n",
       " 'tritt',\n",
       " 'trotzdem',\n",
       " 'tun',\n",
       " 'u',\n",
       " 'uhr',\n",
       " 'um',\n",
       " 'und',\n",
       " 'und?',\n",
       " 'uns',\n",
       " 'unse',\n",
       " 'unsem',\n",
       " 'unsen',\n",
       " 'unser',\n",
       " 'unsere',\n",
       " 'unserer',\n",
       " 'unses',\n",
       " 'unter',\n",
       " 'v',\n",
       " 'vergangenen',\n",
       " 'viel',\n",
       " 'viele',\n",
       " 'vielem',\n",
       " 'vielen',\n",
       " 'vielleicht',\n",
       " 'vier',\n",
       " 'vierte',\n",
       " 'vierten',\n",
       " 'vierter',\n",
       " 'viertes',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'w',\n",
       " 'wahr?',\n",
       " 'wann',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'wart',\n",
       " 'warum',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'wegen',\n",
       " 'weil',\n",
       " 'weit',\n",
       " 'weiter',\n",
       " 'weitere',\n",
       " 'weiteren',\n",
       " 'weiteres',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wem',\n",
       " 'wen',\n",
       " 'wenig',\n",
       " 'wenige',\n",
       " 'weniger',\n",
       " 'weniges',\n",
       " 'wenigstens',\n",
       " 'wenn',\n",
       " 'wer',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'werdet',\n",
       " 'weshalb',\n",
       " 'wessen',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'wieso',\n",
       " 'will',\n",
       " 'willst',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirklich',\n",
       " 'wirst',\n",
       " 'wissen',\n",
       " 'wo',\n",
       " 'woher',\n",
       " 'wohin',\n",
       " 'wohl',\n",
       " 'wollen',\n",
       " 'wollt',\n",
       " 'wollte',\n",
       " 'wollten',\n",
       " 'worden',\n",
       " 'wurde',\n",
       " 'wurden',\n",
       " 'während',\n",
       " 'währenddem',\n",
       " 'währenddessen',\n",
       " 'wäre',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'z.b',\n",
       " 'zehn',\n",
       " 'zehnte',\n",
       " 'zehnten',\n",
       " 'zehnter',\n",
       " 'zehntes',\n",
       " 'zeit',\n",
       " 'zu',\n",
       " 'zuerst',\n",
       " 'zugleich',\n",
       " 'zum',\n",
       " 'zunächst',\n",
       " 'zur',\n",
       " 'zurück',\n",
       " 'zusammen',\n",
       " 'zwanzig',\n",
       " 'zwar',\n",
       " 'zwei',\n",
       " 'zweite',\n",
       " 'zweiten',\n",
       " 'zweiter',\n",
       " 'zweites',\n",
       " 'zwischen',\n",
       " 'zwölf',\n",
       " 'über',\n",
       " 'überhaupt',\n",
       " 'übrigens',\n",
       " 'hallo',\n",
       " 'gut',\n",
       " 'gute',\n",
       " 'guten',\n",
       " 'guter',\n",
       " 'gutes',\n",
       " 'gutem',\n",
       " 'danke',\n",
       " 'dank']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_REMOVE = re.compile(r'\\b(?:%s)\\b' % '|'.join(STOPWORDS),re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare(text):\n",
    "    \n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    # lowercase text\n",
    "    #text = text.lower()\n",
    "    # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = BAD_SYMBOLS_RE.sub('',text)\n",
    "    text = text.lower()\n",
    "    text = PHRASES.sub('',text)\n",
    "    # delete stopwords from text\n",
    "    text = STOPWORDS_REMOVE.sub('',text)\n",
    "    \n",
    "    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text) # delete words that are occuring multiple times in a row\n",
    "    text =  re.sub(r\"\\b[a-zA-ZÖÄÜöäüß]\\b\", \"\", text) # delete single lettre words\n",
    "    text = re.sub(' +', ' ', text) # delete multiple spaces\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titles_prepare(text,start):\n",
    "    \n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    # lowercase text\n",
    "    #text = text.lower()\n",
    "    # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub(r\"([?.!,¿-])\", r\" \\1 \", text) # keep the punctuation\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    #text = text.lower()\n",
    "    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text) # delete words that are occuring multiple times in a row\n",
    "    text =  re.sub(r\"\\b[a-zA-ZÖÄÜöäüß]\\b\", \"\", text) # delete single lettre words\n",
    "    text = re.sub(' +', ' ', text) # delete multiple spaces\n",
    "    if start:\n",
    "        return '<start> ' + text.lower() # for teacher forcing\n",
    "    else:\n",
    "        return text.lower() + ' <stop>'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess in-place\n",
    "def process_text(text_array,n):\n",
    "    for i in range(len(text_array)):\n",
    "        X[n*10000+i] = text_prepare(text_array[i])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess in-place\n",
    "def process_titles(text_array,n):\n",
    "    text_array_ = deepcopy(text_array)\n",
    "    for i in range(len(text_array)):\n",
    "        titles[n*10000+i] = titles_prepare(text_array_[i],True)\n",
    "        titles_output[n*10000+i] = titles_prepare(text_array_[i],False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing X took :  364.50031185150146  seconds\n"
     ]
    }
   ],
   "source": [
    "# do it in parallel stop programming like the first man on earth... \n",
    "thread_list = []\n",
    "start = time()\n",
    "for i in range(10):\n",
    "    thread_list.append(threading.Thread(target = process_text,args = ((X[i*10000:i*10000+10000]),i)))\n",
    "    thread_list[i].start()\n",
    "for thread in thread_list:\n",
    "    thread.join()\n",
    "stop = time()\n",
    "print('preprocessing X took : ', stop-start,' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' existiert gegründetes gewerbe einzelunternehmer fahrzeug geleast komplett abgesetzt sämtliche leasingrate leasingsonderzahlung benzin vorsteuer usw bank leasingangebot privat ausstellen gewerblich gegründete gewerbe vorliegen privatperson abgeschlossener leasingvertrag vollständig gewerbe eingebracht abzusetzen leasingvertrag ausdrücklich privatleasing gewerblichen leasingvertrag falls gäbe risiko falle betriebsprüfung leasingvertrag privatleasing deklariert sodass betriebsausgaben anerkannt '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing titles took :  4.610769748687744  seconds\n"
     ]
    }
   ],
   "source": [
    "# do it in parallel stop programming like the first man on earth... \n",
    "thread_list = []\n",
    "start = time()\n",
    "for i in range(10):\n",
    "    thread_list.append(threading.Thread(target = process_titles,args = ((titles[i*10000:i*10000+10000]),i)))\n",
    "    thread_list[i].start()\n",
    "for thread in thread_list:\n",
    "    thread.join()\n",
    "stop = time()\n",
    "print('preprocessing titles took : ', stop-start,' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> privates leasing im gewerbe ? '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare texts for use in a deep learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle titles, lables and questions\n",
    "dataset = list(zip(X,titles,titles_output,y))\n",
    "random.shuffle(dataset)\n",
    "mixed_X, mixed_titles,mixed_titles_output, mixed_y= zip(*dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:   geburt deutscher staatsbürger ehefrau mexikannerin ehefrau befindet zweimonatigen intensiven deutschkurs offiziellen goethe instituts mexico intensiven deutschkurs verlangt familienleben führen absolvieren job ingenieurin gekündigt fahrtzeit eingerechnet täglich stunden erlernen einfachen deutschkenntnisse angemessene krankenversicherung hochzeit defekt gallenblase festgestellt operative entfernung gallenblase jederzeit meist tödlich verlaufender gallenblasenriss entstehen ärztliche diagnose ausreichenden finanziellen operation mexikanischen krankenhaus westlichem niveau arbeitet staatlichen krankenhäuser mexico befinden schrecklichen hygienischen fachlichen technischen zustand notwendig schnell aufenthaltserlaubnis bekommt krankenversicherung aufnehmen operiert deutschprüfung dauer testergebnis deutschprüfung feststeht monate vergehen verlängerte visabearbeitungszeit monaten aufgrund flüchtlingskrise studierte ingenieurin leider dauert monate offiziellen abschluss privatuniversität bekommt uni vollständig bezahlt krankheit visaantrag erwähnen könnten nachteile entstehen hochzeit organisation hochzeit wochen gedauert unterstellen krankenversicherung heiraten agumentieren schnell visum bekommt schwierigen test bestehen namhaftes deutsches unternehmen mexiko gearbeitet hervoragenden schulabschluss sogar dozentin hochschule tätig ehrenamtlicher deutschlehrer flüchtlinge erfahrung integration ausländern \n",
      "\n",
      "title: <start> schnelleres visum zum nachzug des ehegatten , wegen lebensbedrohlicher krankheit\n",
      "\n",
      "title output: schnelleres visum zum nachzug des ehegatten , wegen lebensbedrohlicher krankheit <stop>\n",
      "\n",
      "area:  Oeffentlichesrecht\n"
     ]
    }
   ],
   "source": [
    "print('question: ',mixed_X[1])\n",
    "print('\\ntitle: '+mixed_titles[1])\n",
    "print('\\ntitle output: '+mixed_titles_output[1])\n",
    "print('\\narea: ',mixed_y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximal length of a question 2840 words\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN_Q = len(max(X, key=lambda x: len(x.split())).split())\n",
    "print('maximal length of a question {} words'.format(MAX_LEN_Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximal length of a title 29 words\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN_T = len(max(titles, key=lambda x: len(x.split())).split())\n",
    "print('maximal length of a title {} words'.format(MAX_LEN_T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = mixed_X[:99000]\n",
    "train_y = mixed_y[:99000]\n",
    "train_titles_input = mixed_titles[:99000]\n",
    "train_titles_output = mixed_titles_output[:99000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = mixed_X[99000:]\n",
    "test_y = mixed_y[99000:]\n",
    "test_titles_input = mixed_titles[99000:]\n",
    "test_titles_output = mixed_titles_output[99000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:   liebe gemeinde folgenden vorfall erlitten meinung mithilfe monaten anfang september gebrauchtwagen skoda octavia freien händler zugelegt letzte urlaub fahrbetrieb kfz fällt elektronik bedeutet auto gefahren fahrtrichtungsanzeiger etc funktionierte auto demnach verkehrssicher km heimatort entfernt rücksprache versicherung adac nächste fachwerkstatt gleichen ort geleitet bordsteuergerät glaube hieß verdacht getauscht keinerlei fehler festzustellen symptomatik beschriebenen probleme hinwies kostenpunkt hause blieb autobahn selben problem liegen nächstgelegenen fachwerkstatt geschleppt anderthalb mitgeteilt bordsteuergerät ausfall schuld gateway diagnoseinterface wasser kfz läuft betroffenen teile laufe korrodierten lasse überprüfen wasser auto läuft werkstatt sowieso halbe konsole auseinander gebaut kostenpunkt entschuldigt elend langen text anliegen verkäufer wagens gewährleistung agbs geregelt zudem klausel informieren fragen werdegang reparatur leider gedanken agbs kopf auto werkstatt angerufen meinte reparaturkosten übernehmen auto km hause transportieren solle kfz reparatur hinstellen genau punkt einsetzen rechtens reparatur übernimmt dennoch rechnungen hinlegen aufkommen nähe heimatortes irgendwelche gerichtsentscheide sachverhalte entschieden nachgeschaut passendes finden lieben mithilfe grüße ronny editiert sorbi \n",
      "\n",
      "title: <start> gewährleistung händler bei fremdwerkstatt - kosten\n",
      "\n",
      "title output: gewährleistung händler bei fremdwerkstatt - kosten <stop>\n",
      "\n",
      "area:  Kaufrecht\n"
     ]
    }
   ],
   "source": [
    "print('question: ',test_X[1])\n",
    "print('\\ntitle: '+test_titles_input[1])\n",
    "print('\\ntitle output: '+test_titles_output[1])\n",
    "print('\\narea: ',test_y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 299331 unique input tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_inputs = Tokenizer(num_words=150000,filters = '') #filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "tokenizer_inputs.fit_on_texts(train_X)\n",
    "train_sequences = tokenizer_inputs.texts_to_sequences(train_X)\n",
    "train_sequences = pad_sequences(train_sequences,maxlen=700)\n",
    "test_sequences = tokenizer_inputs.texts_to_sequences(test_X)\n",
    "test_sequences = pad_sequences(test_sequences,maxlen=700)\n",
    "# get the word to index mapping for input language\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print('Found %s unique input tokens.' % len(word2idx_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_X = {}\n",
    "for key, value in word2idx_inputs.items():\n",
    "    idx2word_X[value]=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('word2idx_X_generator.pkl','wb')\n",
    "pickle.dump(word2idx_inputs,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('idx2word_X_generator.pkl','wb')\n",
    "pickle.dump(idx2word_X,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   200,  1015,  2276],\n",
       "       [    0,     0,     0, ...,  1443, 12160, 15680],\n",
       "       [    0,     0,     0, ...,   577,  1094,   223],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  2335,   747,  1352],\n",
       "       [    0,     0,     0, ...,   557,   303,  1044],\n",
       "       [    0,     0,     0, ...,   555,   387,   165]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train_padded_generator.npy',np.array(train_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_test_padded_generator.npy',np.array(test_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 47623 unique input tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_inputs = Tokenizer(num_words=30000) #filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "tokenizer_inputs.fit_on_texts(train_titles_input+train_titles_output)\n",
    "\n",
    "input_sequences_input = tokenizer_inputs.texts_to_sequences(train_titles_input)\n",
    "input_sequences_input = pad_sequences(input_sequences_input,maxlen=20,padding = 'post')\n",
    "input_sequences_output = tokenizer_inputs.texts_to_sequences(train_titles_output)\n",
    "input_sequences_output = pad_sequences(input_sequences_output,maxlen=20,padding = 'post')\n",
    "\n",
    "test_sequences_input =  tokenizer_inputs.texts_to_sequences(test_titles_input)\n",
    "test_sequences_input = pad_sequences(test_sequences_input,maxlen=20, padding = 'post')\n",
    "test_sequences_output =  tokenizer_inputs.texts_to_sequences(test_titles_output)\n",
    "test_sequences_output = pad_sequences(test_sequences_output,maxlen=20, padding = 'post')\n",
    "# get the word to index mapping for input language\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print('Found %s unique input tokens.' % len(word2idx_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,  6115,   369, 14192,    46,  1890,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6115,   369, 14192,    46,  1890,     2,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_titles = {}\n",
    "for key, value in word2idx_inputs.items():\n",
    "    idx2word_titles[value]=key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save preprocessed data to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('word2idx_titles_generator.pkl','wb')\n",
    "pickle.dump(word2idx_inputs,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('idx2word_titles_generator.pkl','wb')\n",
    "pickle.dump(idx2word_titles,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('titles_train_input_padded_generator.npy',np.array(input_sequences_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('titles_train_output_padded_generator.npy',np.array(input_sequences_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('titles_test_input_padded_generator.npy',np.array(test_sequences_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('titles_test_output_padded_generator.npy',np.array(test_sequences_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y_train_generator.npy',np.array(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y_test_generator.npy',np.array(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save word2vec as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 151456 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# store all the pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open(os.path.join('trained_vectorsW2V.csv')) as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "    for line in f:\n",
    "        values = line.split(',')\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('word2idx_X_generator.pkl','rb')\n",
    "word2idx_X = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 150000\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print('Filling pre-trained embeddings...')\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_X.items():\n",
    "    if i < num_words:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all zeros.\n",
    "          embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embeddings_generator.npy',embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
